# TF-IDF

## 1. TF-IDFの計算方法
例文  

1.砂糖控え目のプリン。よく煮詰めたカラメルをのせる。カラメルがほろ苦くておいしい。

　2.カスタードたっぷり。パイ生地にお砂糖を振りかけて出来上がり。

　3.生クリームは砂糖少々でホイップしたものを使う。

TF-IDFを計算(logは自然対数(ln)を使う)

文書1.  
  カラメル : TF-IDF = 2 x ln(3/1) = 2 × 1.09 = 2.20<- 出現頻度が高く、かつ文書１にしか出現しないので値がおおきい  
  カスタード：TF-IDF = 1 x ln(3/1) = 1 × 1.09 = 1.09 <- 出現頻度は低いが、文書２にしか出現しないので値が大きい  
  砂糖：TF-IDF = 3 x ln(3/3) = 0 <- 　砂糖はありふれているので対数値が0になる！

### 演習：以下の語彙のTF-IDFを計算せよ

文書2.  
カスタード：   
     砂糖：  

## 2. TF-IDFを特徴量とするBoW（演習）
[tsukurepo_bow_vectorizer.py](tsukurepo_bow_vectorizer.py)を修正してTF-IDFの値でBoWデータセットを作成せよ。次に、このデータセットにもとづき再度k nearest neighborで学習を行い、識別精度を確認せよ。(tf-idf vectorizerでググると答えが見つかります)   

- TF-IDFは、汎用語彙のフィルタリングと同じ効果もある（なぜだろうか）。
- ので、多次元のままで識別精度が向上するが、低頻度語彙をフィルタリングするとさらに向上するはず。
- 


